---
title: "Evaluaciones autom√°ticas"
description: "Analiza el rendimiento de tus agentes mediante evaluaciones autom√°ticas generadas por Devic, basadas en el enfoque LLM-as-Judge."
---

# Rendimiento del agente

Devic incluye un sistema de **evaluaci√≥n autom√°tica del rendimiento** que analiza el comportamiento de los agentes al finalizar cada ejecuci√≥n.  
Este m√≥dulo aplica m√©tricas predefinidas que permiten medir de forma objetiva la precisi√≥n, planificaci√≥n, ejecuci√≥n y finalizaci√≥n de las tareas.

![Panel de evaluaci√≥n autom√°tica de ejecuci√≥n](9147b810-ea00-444a-9b63-2d7cd2f23c37.png)

---

## Evaluaciones predefinidas

Las evaluaciones vienen **configuradas por defecto** en la plataforma e incluyen indicadores clave del desempe√±o del agente:

| Indicador | Descripci√≥n |
|------------|-------------|
| **Instruction Following** | Eval√∫a el grado de cumplimiento de las instrucciones proporcionadas. |
| **Task Planning** | Mide la calidad y coherencia en la planificaci√≥n de tareas. |
| **Task Execution** | Analiza la precisi√≥n y consistencia de la ejecuci√≥n. |
| **Tool Usage** | Eval√∫a el uso eficiente de las herramientas disponibles. |
| **Finalization** | Verifica que el agente cierre correctamente el flujo de trabajo. |

Cada m√©trica se punt√∫a en una escala del **0 al 10**, generando un **Overall Score** (puntuaci√≥n global) que resume el rendimiento general de la ejecuci√≥n.

---

## Evaluaciones personalizadas

Adem√°s de las m√©tricas predefinidas, es posible **crear tus propias evaluaciones personalizadas** para adaptarlas a los objetivos o criterios espec√≠ficos de tu organizaci√≥n.  

Estas configuraciones se gestionan desde la secci√≥n:

**Other Options ‚Üí Evaluation Configuration**

üëâ [Ver configuraci√≥n de evaluaciones personalizadas](https://devic.mintlify.app/devic/agents/other-options)

All√≠ puedes definir nuevos criterios, ajustar ponderaciones o incorporar indicadores adicionales seg√∫n las necesidades de tu flujo operativo.

---

## LLM as Judge

Devic implementa el enfoque **LLM-as-Judge**, en el que **un modelo de lenguaje adicional act√∫a como evaluador** del rendimiento del agente.  
Este modelo analiza los resultados generados, interpreta la coherencia de las acciones y emite una puntuaci√≥n basada en criterios definidos.

Gracias a este sistema, las evaluaciones son:

- **Objetivas**, al provenir de un evaluador externo al agente ejecutor.  
- **Consistentes**, ya que aplican las mismas reglas de an√°lisis en cada ejecuci√≥n.  
- **Automatizadas**, eliminando la necesidad de revisi√≥n manual.  
- **Explicativas**, con res√∫menes interpretativos que describen fortalezas y √°reas de mejora.

![Evaluaci√≥n con puntuaci√≥n general y resumen de desempe√±o](9147b810-ea00-444a-9b63-2d7cd2f23c37.png)

---

## Interpretaci√≥n de resultados

El panel de evaluaci√≥n muestra un resumen detallado que incluye:

- **Overall Performance:** calificaci√≥n general (por ejemplo, *Excellent*, *Good*, *Needs Improvement*).  
- **Summary:** an√°lisis textual generado por el modelo evaluador, con observaciones sobre el desempe√±o.  
- **Strong Areas:** n√∫mero de aspectos destacados.  
- **Areas to Improve:** n√∫mero de puntos de mejora detectados.

Adem√°s, el bot√≥n **‚ÄúGet Suggestions‚Äù** permite solicitar recomendaciones autom√°ticas para optimizar el comportamiento del agente en futuras ejecuciones.

---

<Note>
El sistema de evaluaci√≥n autom√°tica de Devic combina la precisi√≥n del an√°lisis cuantitativo con la interpretaci√≥n cualitativa de un modelo de lenguaje, proporcionando una visi√≥n integral del rendimiento del agente.
</Note>

---

## Pr√≥ximos pasos

<CardGroup cols={2}>
  <Card title="Human in the Loop" icon="user-check" href="./human-in-the-loop">
    Aprende c√≥mo incluir validaci√≥n humana dentro del flujo de ejecuci√≥n para reforzar el control operativo.
  </Card>
  <Card title="Configuraci√≥n avanzada" icon="settings" href="../other-options">
    Personaliza las m√©tricas y criterios de evaluaci√≥n de tus agentes.
  </Card>
</CardGroup>
